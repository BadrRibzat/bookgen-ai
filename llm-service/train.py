#!/usr/bin/env python3
"""Simple CPU-friendly fine-tuning pipeline for BookGen domain datasets.

This script loads the processed JSON files generated by ``process_domain_data.py``,
formats them into instruction/completion style prompts, and fine-tunes a
causal language model (GPT-2/DistilGPT-2, etc.) using Hugging Face Transformers.

Example usage (distilgpt2 on CPU):

    python train.py \
        --model-name distilgpt2 \
        --output-dir runs/distilgpt2-bookgen \
        --domains ai_ml ecommerce healthtech \
        --epochs 3 \
        --batch-size 2 \
        --gradient-accumulation 8 \
        --max-samples 2000

The defaults are conservative so the loop can run on CPU-only hardware. Adjust
hyperparameters to match available resources. The script saves model
checkpoints, tokenizer files, and a small training summary under ``output-dir``.
"""

from __future__ import annotations

import argparse
import json
import logging
import math
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence

import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
    set_seed,
)

LOGGER = logging.getLogger("bookgen_train")
LLM_SERVICE_ROOT = Path(__file__).resolve().parent
DEFAULT_DATA_ROOT = LLM_SERVICE_ROOT / "data" / "training_sets"


@dataclass
class TrainingExample:
    """A single formatted example ready for tokenisation."""

    text: str
    domain: str
    source_file: Path


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Fine-tune a causal LM on BookGen processed datasets")
    parser.add_argument(
        "--data-root",
        type=Path,
        default=DEFAULT_DATA_ROOT,
        help="Root directory containing domain training_sets (defaults to llm-service/data/training_sets)",
    )
    parser.add_argument(
        "--domains",
        nargs="*",
        help="Optional subset of domains to train on (e.g. ai_ml ecommerce). Defaults to all discovered domains.",
    )
    parser.add_argument(
        "--model-name",
        default="distilgpt2",
        help="Base model checkpoint from Hugging Face Hub (e.g. distilgpt2, gpt2, gpt2-medium)",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        required=True,
        help="Directory where checkpoints and tokenizer artefacts will be written",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=3,
        help="Number of training epochs",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=2,
        help="Per-device (CPU) batch size",
    )
    parser.add_argument(
        "--gradient-accumulation",
        type=int,
        default=8,
        help="Gradient accumulation steps to emulate larger batches on CPU",
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=5e-5,
        help="Initial learning rate",
    )
    parser.add_argument(
        "--weight-decay",
        type=float,
        default=0.0,
        help="Weight decay",
    )
    parser.add_argument(
        "--warmup-ratio",
        type=float,
        default=0.03,
        help="Linear warmup ratio",
    )
    parser.add_argument(
        "--eval-split",
        type=float,
        default=0.05,
        help="Fraction of examples reserved for evaluation (0 disables eval)",
    )
    parser.add_argument(
        "--max-samples",
        type=int,
        help="Optional cap on total examples across all domains (useful for quick experiments)",
    )
    parser.add_argument(
        "--max-seq-length",
        type=int,
        default=1024,
        help="Maximum tokenised sequence length",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for deterministic sampling",
    )
    parser.add_argument(
        "--num-proc",
        type=int,
        default=1,
        help="Number of CPU workers for tokenisation",
    )
    parser.add_argument(
        "--include-context",
        action="store_true",
        help="When set, include the example context field in the formatted prompt",
    )
    parser.add_argument(
        "--summary-file",
        type=Path,
        default=None,
        help="Optional path to write a JSON summary of the training run",
    )
    parser.add_argument(
        "--logging-steps",
        type=int,
        default=50,
        help="Frequency (in steps) for loss logging",
    )
    parser.add_argument(
        "--save-steps",
        type=int,
        default=500,
        help="Checkpoint save frequency (in steps)",
    )
    parser.add_argument(
        "--save-total-limit",
        type=int,
        default=3,
        help="Maximum number of checkpoints to keep on disk",
    )
    return parser.parse_args()


def discover_domains(data_root: Path) -> List[str]:
    domains = [p.name for p in sorted(data_root.iterdir()) if p.is_dir()]
    LOGGER.info("Discovered domains: %s", ", ".join(domains))
    return domains


def iter_training_files(data_root: Path, domains: Sequence[str]) -> Iterable[tuple[str, Path]]:
    for domain in domains:
        processed_dir = data_root / domain / "processed"
        if not processed_dir.exists():
            LOGGER.warning("Processed directory missing for %s: %s", domain, processed_dir)
            continue
        for json_file in sorted(processed_dir.glob("*.json")):
            if json_file.name.upper() == "SUMMARY.json":
                continue
            yield domain, json_file


def build_text(prompt: str, completion: str, context: Optional[str]) -> str:
    segments: List[str] = []
    if context and context.strip():
        segments.append(f"[CONTEXT]\n{context.strip()}")
    segments.append(f"[PROMPT]\n{prompt.strip()}")
    segments.append(f"[RESPONSE]\n{completion.strip()}")
    return "\n\n".join(segments)


def load_examples(
    data_root: Path,
    domains: Sequence[str],
    include_context: bool,
    max_samples: Optional[int],
) -> List[TrainingExample]:
    examples: List[TrainingExample] = []
    for domain, json_file in iter_training_files(data_root, domains):
        with json_file.open("r", encoding="utf-8") as handle:
            payload = json.load(handle)
        training_examples = payload.get("training_examples", [])
        for example in training_examples:
            prompt = example.get("input", "").strip()
            completion = example.get("output", "").strip()
            context = example.get("context", "") if include_context else ""
            if not prompt or not completion:
                continue
            text = build_text(prompt, completion, context)
            examples.append(TrainingExample(text=text, domain=domain, source_file=json_file))
            if max_samples and len(examples) >= max_samples:
                LOGGER.info("Reached max samples limit (%s)", max_samples)
                return examples
    return examples


def split_dataset(records: List[TrainingExample], eval_split: float, seed: int) -> tuple[List[TrainingExample], List[TrainingExample]]:
    if not records:
        raise ValueError("No training examples found. Did you run process_domain_data.py?")
    rng = random.Random(seed)
    rng.shuffle(records)
    if eval_split <= 0.0 or len(records) < 2:
        return records, []
    eval_size = max(1, int(len(records) * eval_split))
    eval_records = records[:eval_size]
    train_records = records[eval_size:]
    if not train_records:
        raise ValueError("Evaluation split too large; no training data remaining")
    return train_records, eval_records


def prepare_dataset(records: List[TrainingExample]) -> Dataset:
    return Dataset.from_list([{"text": example.text} for example in records])


def main() -> None:
    args = parse_args()
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    set_seed(args.seed)
    torch.set_num_threads(max(1, torch.get_num_threads()))  # Ensure CPU execution respects available cores

    if not args.output_dir.exists():
        args.output_dir.mkdir(parents=True, exist_ok=True)

    if args.domains:
        domains = list(args.domains)
    else:
        domains = discover_domains(args.data_root)
    if not domains:
        raise SystemExit("No domain folders discovered under data-root")

    LOGGER.info("Loading processed examples from %s", args.data_root)
    raw_examples = load_examples(
        data_root=args.data_root,
        domains=domains,
        include_context=args.include_context,
        max_samples=args.max_samples,
    )
    LOGGER.info("Loaded %s examples across %s domains", len(raw_examples), len(domains))

    train_examples, eval_examples = split_dataset(raw_examples, args.eval_split, args.seed)
    train_dataset = prepare_dataset(train_examples)
    eval_dataset = prepare_dataset(eval_examples) if eval_examples else None

    LOGGER.info("Initialising tokenizer and model: %s", args.model_name)
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(args.model_name)
    model.resize_token_embeddings(len(tokenizer))

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    def tokenize_batch(batch: Dict[str, List[str]]) -> Dict[str, List[int]]:
        return tokenizer(
            batch["text"],
            truncation=True,
            max_length=args.max_seq_length,
            padding="max_length",
        )

    train_dataset = train_dataset.map(tokenize_batch, batched=True, num_proc=args.num_proc)
    train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])

    if eval_dataset is not None:
        eval_dataset = eval_dataset.map(tokenize_batch, batched=True, num_proc=args.num_proc)
        eval_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])

    total_train_steps = math.ceil(len(train_dataset) / (args.batch_size * args.gradient_accumulation)) * args.epochs
    LOGGER.info(
        "Training set size: %s | Eval set size: %s | Estimated total steps: %s",
        len(train_dataset),
        len(eval_dataset) if eval_dataset is not None else 0,
        total_train_steps,
    )

    evaluation_strategy = "steps" if eval_dataset is not None else "no"
    training_args = TrainingArguments(
        output_dir=str(args.output_dir),
        overwrite_output_dir=True,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        evaluation_strategy=evaluation_strategy,
        logging_strategy="steps",
        save_strategy="steps",
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_ratio=args.warmup_ratio,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        report_to="none",
        load_best_model_at_end=eval_dataset is not None,
        metric_for_best_model="loss",
        greater_is_better=False,
        no_cuda=True,  # Enforce CPU execution
        dataloader_num_workers=4, # Use multiple CPU workers
        dataloader_pin_memory=True,
        torch_compile=False,    # Disable as it can slow down CPU training
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )

    train_result = trainer.train()
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)

    metrics = train_result.metrics
    if eval_dataset is not None:
        eval_metrics = trainer.evaluate()
        metrics.update({f"eval_{k}": v for k, v in eval_metrics.items()})

    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    trainer.save_state()

    if args.summary_file:
        summary_payload = {
            "model": args.model_name,
            "output_dir": str(args.output_dir.resolve()),
            "domains": domains,
            "num_train_examples": len(train_dataset),
            "num_eval_examples": len(eval_dataset) if eval_dataset is not None else 0,
            "metrics": metrics,
        }
        summary_path = args.summary_file.resolve()
        summary_path.parent.mkdir(parents=True, exist_ok=True)
        with summary_path.open("w", encoding="utf-8") as handle:
            json.dump(summary_payload, handle, indent=2)
        LOGGER.info("Training summary written to %s", summary_path)

    LOGGER.info("Training complete. Model artefacts saved to %s", args.output_dir)


if __name__ == "__main__":
    main()

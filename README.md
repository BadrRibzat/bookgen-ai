# üìö BookGen-AI

> **AI-Powered SaaS Book Generation Platform**  
> Transform your ideas into professionally formatted books with the power of artificial intelligence.

## üë®‚Äçüíª About the Developer

**Badr Ribzat** - Self-Taught Full-Stack Software Engineer from Morocco

üìß **Contact**: [badrribzat@gmail.com](mailto:badrribzat@gmail.com)  
üåê **Portfolio**: [https://badr-portfolio.vercel.app](https://badr-portfolio.vercel.app)

### Professional Background
- **Self-taught Full-Stack Software Engineer** available for opportunities and collaborations
- **Experience across multiple domains**: Biomedical, Teaching IT, AI/Chatbot Development
- **Notable Projects**: 
  - Biomedical-AI Human Body Detection System
  - IT-Learning Platform
  - Internationalization Portfolio 
  - Resume Generator SaaS Platform
  - AI-Powered Chatbot Systems

ü§ù **Available for**: Opportunities, collaborations, and technical discussions with companies across various industries

---

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![Django](https://img.shields.io/badge/Django-5.0+-green.svg)](https://www.djangoproject.com/)
[![Next.js](https://img.shields.io/badge/Next.js-14+-black.svg)](https://nextjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue.svg)](https://www.typescriptlang.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.9+-red.svg)](https://pytorch.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-Atlas-green.svg)](https://www.mongodb.com/)

---

## üåü Features

### ‚úÖ Current Implementation (Production Ready)

#### üîê **Complete Authentication System**
- JWT-based authentication with refresh tokens
- Email verification and password reset workflows
- Secure user registration with validation
- Session management and logout functionality

#### üë§ **Advanced User Management**
- Extended user profiles with subscription tiers
- User analytics and activity tracking
- Profile customization and preferences
- Role-based access control

#### üìñ **Intelligent Book Generation**
- Multi-step book creation wizard with progress tracking
- Real-time form validation with Zod schemas
- Genre-specific content templates
- Professional book metadata management

#### ü§ñ **Custom LLM Training Service**
- **Local JSON Storage Infrastructure** - Complete 12-domain data organization
- **Manual Data Collection Workflow** - Quality-controlled training data
- **Subscription-Tier Support** - Basic, Professional, Enterprise content generation
- **MongoDB Atlas Integration** - Production-ready database storage
- **GPT-2 Fine-tuning** - Domain-specific model customization
- **Data Validation Tools** - Comprehensive JSON validation and quality checks
- **Background Training Pipeline** - Async model training with progress tracking

#### üìÑ **Professional PDF Generation**
- High-quality book formatting with ReportLab
- Custom templates and styling options
- Automated table of contents and indexing
- Professional typography and layout

#### üèóÔ∏è **Robust Infrastructure**
- Celery async task processing
- Redis message broker and caching
- Comprehensive error handling
- Production-ready logging and monitoring

#### üîê **Admin Management Dashboard**
- Secure admin-only access at `/management-secure`
- User management and analytics
- System-wide statistics and monitoring
- Book management across all users
- Subscription and revenue tracking


## üß† Fine-Tuned Model (November 2025)

- **Kaggle GPU Training Pipeline**: distilgpt2 fine-tuned on 144,699 curated examples across 12 publishing domains in **6h 02m** (down from the original 2-year CPU estimate).
- **Model Location**: `llm-service/models/final_model/` (‚âà300‚ÄØMB, cached in Docker images and mounted read-only in containers).
- **Performance**: Validation perplexity `6.83` vs `14.71` baseline; average generation latency `381‚ÄØms`; domain keyword relevance ‚â•‚ÄØ0.75 across all tracks.
- **Usage**: Export `LLM_MODEL_PATH` (defaults to `/app/models/final_model`) and call `LLMTrainer` or the new QA scripts for immediate inference.
- **Test Coverage**: Comprehensive pytest suite exercises model loading, domain-specific generation, performance benchmarks, and real-world prompt scenarios for all 12 domains.
- **Deployment Tooling**: `llm-service/scripts/validate_model.py`, `benchmark_model.py`, and `quality_assurance.py` ensure reproducible validation in CI/CD.

### üîÑ Coming Soon
- **Manual Data Collection** - Research and collect high-quality training data
- **Rich Book Editing** - Advanced text editor for content refinement
- **AI Cover Generation** - Automated professional book covers  
- **PDF Browser Preview** - In-browser PDF viewing and downloading
- **Payment Integration** - Subscription and billing management
- **Multi-language Support** - Internationalization features

---

## üèóÔ∏è System Architecture

```mermaid
graph TB
    subgraph "Frontend Layer"
        A[Next.js Frontend] --> B[TypeScript Components]
        B --> C[Tailwind CSS Styling]
        C --> D[Authentication Context]
    end
    
    subgraph "API Gateway"
        E[Django REST API] --> F[JWT Authentication]
        F --> G[User Management]
        G --> H[Book CRUD Operations]
    end
    
    subgraph "AI Processing Layer"
        I[FastAPI LLM Service] --> J[Local JSON Storage]
        J --> K[Data Validation Tool]
        K --> L[MongoDB Atlas Storage]
        L --> M[GPT-2 Fine-tuning]
        M --> N[Subscription-Tier Generation]
        N --> O[Domain-Specific Models]
    end
    
    subgraph "Data & Infrastructure"
        P[PostgreSQL] --> Q[User Data]
        R[MongoDB Atlas] --> S[Training Data]
        T[Redis] --> U[Caching & Tasks]
        V[Celery] --> W[Async Processing]
    end
    
    A --> E
    E --> I
    I --> R
    E --> P
    E --> T
    T --> V
```

---

## üìÅ Project Structure

```
bookgen-ai/
‚îú‚îÄ‚îÄ üé® frontend/                    # Next.js Application
‚îÇ   ‚îú‚îÄ‚îÄ app/                        # App Router pages
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/                   # Authentication flows
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard/              # User dashboard
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/                    # API routes
‚îÇ   ‚îú‚îÄ‚îÄ components/                 # Reusable UI components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/                   # Auth-specific components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/                     # Base UI components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ layout/                 # Layout components
‚îÇ   ‚îú‚îÄ‚îÄ lib/                        # Utilities and configurations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API client functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contexts/               # React contexts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/                  # Custom React hooks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validation/             # Zod schemas
‚îÇ   ‚îî‚îÄ‚îÄ shared/types/               # TypeScript type definitions
‚îÇ
‚îú‚îÄ‚îÄ üîß backend/                     # Django REST API
‚îÇ   ‚îú‚îÄ‚îÄ apps/                       # Django applications
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ users/                  # User management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/                   # Core functionality
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ books/                  # Book management
‚îÇ   ‚îú‚îÄ‚îÄ config/                     # Django configuration
‚îÇ   ‚îú‚îÄ‚îÄ tests/                      # Backend test suites
‚îÇ   ‚îî‚îÄ‚îÄ templates/                  # Email templates
‚îÇ
‚îú‚îÄ‚îÄ ü§ñ llm-service/                 # FastAPI AI Service
‚îÇ   ‚îú‚îÄ‚îÄ app/                        # FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI app entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml/                     # Machine learning modules
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_schema.py      # MongoDB schemas
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_importer.py    # Local JSON import & validation
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm_trainer.py      # GPT-2 fine-tuning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Pydantic models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/                    # API route handlers
‚îÇ   ‚îú‚îÄ‚îÄ data/training_sets/         # Local JSON storage (12 domains)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md               # Complete data collection guide
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ template.json           # Universal data format template
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cybersecurity/          # Cybersecurity training data
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ template.json       # Domain-specific template
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai_ml/                  # AI/ML training data
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ template.json       # Domain-specific template
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ automation/             # Automation training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ healthtech/             # HealthTech training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ creator_economy/        # Creator Economy training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ web3/                   # Web3 training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ecommerce/              # E-commerce training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_analytics/         # Data Analytics training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gaming/                 # Gaming training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kids_parenting/         # Kids/Parenting training data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nutrition/              # Nutrition training data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recipes/                # Recipes training data
‚îÇ   ‚îú‚îÄ‚îÄ tests/                      # LLM service tests
‚îÇ   ‚îú‚îÄ‚îÄ demo_manual_import.py       # CLI import tool for local JSON
‚îÇ   ‚îú‚îÄ‚îÄ validate_data.py            # JSON validation and quality check tool
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ README_CUSTOM_TRAINING.md   # Complete documentation
‚îÇ   ‚îî‚îÄ‚îÄ IMPLEMENTATION_COMPLETE.md  # Implementation summary
‚îÇ
‚îú‚îÄ‚îÄ üìã shared/                      # Shared resources
‚îÇ   ‚îî‚îÄ‚îÄ types/                      # Common TypeScript types
‚îÇ
‚îú‚îÄ‚îÄ üõ†Ô∏è scripts/                     # Automation scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup.sh                   # Development setup
‚îÇ   ‚îú‚îÄ‚îÄ run-tests.sh               # Test runner
‚îÇ   ‚îî‚îÄ‚îÄ seed-db.sh                 # Database seeding
‚îÇ
‚îú‚îÄ‚îÄ üìã docs/                        # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ LLM_SERVICE_COMPLETE.md    # LLM service documentation
‚îÇ   ‚îî‚îÄ‚îÄ IMPLEMENTATION_CHECKLIST.md # Implementation tracking
‚îÇ
‚îî‚îÄ‚îÄ üê≥ docker-compose.yml           # Development environment
```

---

## üöÄ Quick Start

### Prerequisites
- **Docker & Docker Compose** (recommended for full setup)
- **Node.js 18+** and npm/yarn
- **Python 3.11+** with pip
- **Git** for version control

### üê≥ Automated Docker Setup (Recommended)
```bash
# Clone the repository
git clone https://github.com/BadrRibzat/bookgen-ai.git
cd bookgen-ai

# Set MongoDB password (matching docker-compose expectations)
export MONGODB_PASSWORD="your-secure-password"

# Make setup script executable and run
chmod +x scripts/setup.sh
./scripts/setup.sh

# Start all services
docker-compose up -d

# Verify services are running
docker-compose ps
```

### üîß Manual Development Setup

#### 1. Backend (Django) Setup
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
python manage.py migrate
python manage.py createsuperuser
python manage.py runserver 8000
```

#### 2. Frontend (Next.js) Setup
```bash
cd frontend
npm install
npm run dev
```

#### 3. LLM Service (FastAPI) Setup
```bash
cd llm-service
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Verify local JSON storage infrastructure
ls data/training_sets/

# Start the service
uvicorn app.main:app --reload --host 0.0.0.0 --port 8001
```

#### 4. Celery Services (Background Tasks)

```bash
# Open a new terminal for Celery Worker
cd backend
source venv/bin/activate  # On Windows: venv\Scripts\activate
celery -A config worker --loglevel=info

# Optional: Open another terminal for Celery Beat (scheduled tasks)
cd backend
source venv/bin/activate
celery -A config beat --loglevel=info
```

### üåê Service URLs
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/api/docs/
- **LLM Service**: http://localhost:8001
- **LLM API Docs**: http://localhost:8001/docs
- **Redis**: localhost:6379
- **Celery Worker**: Background service (no URL)
- **Celery Beat**: Background service (no URL)

> [!NOTE]
> The LLM service runs on port **8001** (not 8002). Celery services run as background processes.


---

## üß™ Testing & Quality Assurance

### Run All Tests
```bash
# Automated test runner
./scripts/run-tests.sh

# Individual service tests
cd backend && python manage.py test
cd frontend && npm test
cd llm-service && python test_setup.py

# Fine-tuned model test suite
cd llm-service && pytest tests/
```

### Test Coverage
- **Backend**: Django TestCase, pytest, factory-boy
- **Frontend**: Jest, React Testing Library, Playwright E2E
- **LLM Service**: pytest-asyncio, FastAPI TestClient
- **Integration**: End-to-end workflow testing

#### LLM Model Validation & QA
- `python llm-service/scripts/validate_model.py --model-path llm-service/models/final_model`
- `python llm-service/scripts/benchmark_model.py --device cpu`
- `python llm-service/scripts/quality_assurance.py`
- `python llm-service/scripts/register_finetuned_model.py --database-url <mongodb-uri>`

---

## ü§ñ LLM Service Implementation

### Core Features
- **Local JSON Storage Infrastructure**: Complete 12-domain data organization with templates
- **Manual Data Collection**: Quality-controlled training data collection workflow
- **MongoDB Atlas Integration**: Production database for training data storage
- **GPT-2 Fine-tuning**: Lightweight, efficient model customization
- **Subscription-Tier Support**: Basic, Professional, Enterprise content generation
- **Data Validation Tools**: Comprehensive JSON validation and quality checking
- **Background Training Pipeline**: Async model training with progress tracking

### Data Collection Workflow
```bash
# 1. Read the comprehensive data collection guide
cat llm-service/data/training_sets/README.md

# 2. Validate your training data before importing
cd llm-service
python validate_data.py --all

# 3. Import validated training data
python demo_manual_import.py import-dir \
  --directory data/training_sets/cybersecurity \
  --domain-id cybersecurity

# 4. Train custom model
curl -X POST "http://localhost:8002/train" \
  -H "Content-Type: application/json" \
  -d '{"domain_id": "cybersecurity", "job_name": "Cybersecurity Model Training", "epochs": 3}'

# 5. Generate domain-specific content
curl -X POST "http://localhost:8002/generate" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What are the main cybersecurity threats in 2024?", "domain_id": "cybersecurity"}'
```

### Supported Domains & Templates
All 12 domains have dedicated folder structure with domain-specific JSON templates:

| Domain | Folder | Focus Areas |
|--------|--------|-------------|
| **Cybersecurity** | `cybersecurity/` | Vulnerabilities, threats, security practices |
| **AI & ML** | `ai_ml/` | Machine learning, AI research, implementations |
| **Automation** | `automation/` | RPA, workflow optimization, process improvement |
| **HealthTech** | `healthtech/` | Medical devices, digital health, telemedicine |
| **Creator Economy** | `creator_economy/` | Content monetization, platform strategies |
| **Web3** | `web3/` | Blockchain, cryptocurrency, DeFi, NFTs |
| **E-commerce** | `ecommerce/` | Online retail, marketplaces, conversion optimization |
| **Data Analytics** | `data_analytics/` | Business intelligence, data science, visualization |
| **Gaming** | `gaming/` | Game development, industry trends, monetization |
| **Kids/Parenting** | `kids_parenting/` | Child development, parenting advice, education |
| **Nutrition** | `nutrition/` | Dietary guidance, health optimization, supplements |
| **Recipes** | `recipes/` | Cooking techniques, recipe development, culinary arts |

### Local JSON Storage Format
```json
{
  "domain": "cybersecurity",
  "description": "Training data for cybersecurity domain",
  "version": "1.0.0",
  "total_examples": 100,
  "subscription_tiers": {
    "basic": {
      "system_prompt": "You are a cybersecurity assistant for beginners...",
      "max_complexity": 3,
      "target_audience": "beginners"
    },
    "professional": {
      "system_prompt": "You are a cybersecurity expert...",
      "max_complexity": 7,
      "target_audience": "professionals"  
    },
    "enterprise": {
      "system_prompt": "You are a senior cybersecurity consultant...",
      "max_complexity": 10,
      "target_audience": "enterprise_leaders"
    }
  },
  "training_examples": [
    {
      "id": "cyber_001",
      "input": "What is a SQL injection attack?",
      "output": "SQL injection is a code injection technique...",
      "difficulty_level": 3,
      "subscription_tier": "basic",
      "quality_score": 9.0
    }
  ]
}
```

### ‚úÖ Implementation Example: Cybersecurity Domain

**Real-world data processing implementation with 5 authoritative sources:**

```bash
# Data Sources Integrated (Nov 2025):
# ‚úì NVD CVE Database (97 vulnerability examples)  
# ‚úì MITRE ATT&CK Framework (50 threat intelligence examples)
# ‚úì Ubuntu Security Notices (10 patch management examples)
# ‚úì ArXiv Cryptography Research (28 academic research examples)
# ‚úì Microsoft Security Updates (JSON format support)

# Processing Results:
# - 185 total training examples across all subscription tiers
# - Quality scores: 8.8-9.7/10 across all sources
# - Tier distribution: 5 basic, 125 professional, 55 enterprise

# Implementation Files:
llm-service/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw_sources/cybersecurity/          # Original data files (gitignored)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nvd_cve_2025.json              # NVD vulnerability data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mitre_attack_enterprise.json    # MITRE ATT&CK techniques
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ubuntu_security_notices.xml     # Ubuntu security advisories  
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ arxiv_crypto_papers.xml         # Academic research papers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ microsoft_security_updates.json # Microsoft security updates
‚îÇ   ‚îî‚îÄ‚îÄ training_sets/cybersecurity/        # Processed training data
‚îÇ       ‚îú‚îÄ‚îÄ vulnerabilities_cve_1.json      # 97 CVE examples
‚îÇ       ‚îú‚îÄ‚îÄ threat_intelligence_mitre_1.json # 50 MITRE examples
‚îÇ       ‚îú‚îÄ‚îÄ patch_management_ubuntu_1.json   # 10 Ubuntu examples
‚îÇ       ‚îî‚îÄ‚îÄ security_research_arxiv_1.json   # 28 research examples
‚îú‚îÄ‚îÄ process_cyber_data_fixed.py             # Data processor script
‚îú‚îÄ‚îÄ test_cybersecurity_data.py              # Quality validation
‚îî‚îÄ‚îÄ Cyber-Security.sh                       # Integration automation

# Validation & Quality Check:
python3 test_cybersecurity_data.py
# ‚úÖ 185 training examples ready
# ‚úÖ All subscription tiers represented  
# ‚úÖ High quality scores (8.8-9.7/10)
# ‚úÖ Multiple authoritative data sources
```

**Key Implementation Features:**
- **Multi-format Processing**: Handles JSON (CVE, MITRE), XML (Ubuntu, ArXiv), RSS feeds
- **Intelligent Tier Assignment**: Automatic difficulty scoring based on CVSS, complexity
- **Quality Validation**: Comprehensive data structure and content validation
- **Production Ready**: Fully processed and validated for LLM training

### üì¶ Domain Data Inventory (Nov 2025)
| Domain | Directory | Raw Volume | Primary Formats | Highlight Sources |
|--------|-----------|------------|-----------------|-------------------|
| AI & ML | `ai_ml/` | 148‚ÄØMB | `.parquet`, `.txt`, `.jsonl`, `.csv` | HumanEval, CodeAlpaca, Dolly, reasoning benchmarks |
| Automation | `automation/` | 43‚ÄØMB | `.json` | RPA / workflow automation JSON corpora |
| Creator Economy | `creator_economy/` | 4‚ÄØMB | `.csv`, `.json`, `.html` | Platform analytics & monetisation datasets |
| Cybersecurity | `cybersecurity/` | 1.0‚ÄØGB | `.jsonl`, `.parquet`, `.csv`, `.json` | MITRE ATT&CK, threat feeds, phishing detection |
| Data Analytics | `data_analytics/` | 395‚ÄØMB | `.csv`, `.json`, `.txt`, `.xlsx` | BI dashboards, KPI benchmarks, storytelling corpora |
| E-commerce | `ecommerce/` | 487‚ÄØMB | `.jpg`, `.csv`, `.json` | Retail product imagery & transactional datasets |
| Gaming | `gaming/` | 3.8‚ÄØGB | `.csv`, `.json` | Player telemetry, live ops analytics |
| HealthTech | `healthtech/` | 9.0‚ÄØGB | `.csv`, `.txt`, `.json`, `.parquet` | Wearables, telehealth transcripts, clinical datasets |
| Kids & Parenting | `kids_parenting/` | 3.7‚ÄØMB | `.csv`, `.docx` | Parenting guides & educational content |
| Nutrition | `nutrition/` | 1.5‚ÄØGB | `.csv`, `.xlsx`, `.tsv`, `.json` | Dietary datasets, supplementation research |
| Recipes | `recipes/` | 2.2‚ÄØGB | `.csv` | High-volume recipe & ingredient corpora |
| Web3 | `web3/` | 2.2‚ÄØMB | `.csv` | Blockchain transaction & token analytics |

> **Note:** All high-volume raw assets are ignored from Git by default. Only metadata, templates, and processing scripts are tracked.

### üó∫Ô∏è Processing Roadmap
1. **Domain-specific ETL Scripts** ‚Äì Extend `process_cyber_data_fixed.py` patterns for each domain (AI/ML, automation, healthtech, etc.).
2. **Format Normalisation** ‚Äì Convert CSV/Parquet into instruction-response JSONL, extract text from DOCX/HTML, and add captions or discard non-text assets (e.g., product images).
3. **Quality Labelling** ‚Äì Apply tier/difficulty scores, deduplicate prompts, and redact sensitive information (especially healthcare data).
4. **Validation Pass** ‚Äì Use `validate_data.py --domain <domain>` to confirm schema compliance and generate quality metrics.
5. **Training Execution** ‚Äì Schedule domain training jobs once validated datasets reach coverage goals; log checkpoints and evaluation metrics.

---

## üîê Environment Configuration

### Backend Environment (.env)
```bash
SECRET_KEY=your-secret-key
DEBUG=True
DATABASE_URL=postgresql://user:pass@localhost:5432/bookgen
REDIS_URL=redis://localhost:6379
EMAIL_HOST_USER=your-email@domain.com
EMAIL_HOST_PASSWORD=your-app-password
```

### LLM Service Environment (.env)
```bash
# MongoDB Atlas (shared with backend) - DO NOT commit real credentials
DATABASE_URL=mongodb+srv://<username>:<password>@<cluster>.mongodb.net/?retryWrites=true&w=majority&appName=<app-name>
MONGODB_DB_NAME=bookgen_ai

# Training Configuration
DEFAULT_BATCH_SIZE=4
DEFAULT_LEARNING_RATE=5e-5
DEFAULT_EPOCHS=3
MAX_SEQUENCE_LENGTH=512

# Hardware Settings
DEVICE=auto  # auto, cpu, cuda
USE_FP16=true

# Supported Domains
SUPPORTED_DOMAINS=cybersecurity,ai_ml,automation,healthtech,creator_economy,web3,ecommerce,data_analytics,gaming,kids_parenting,nutrition,recipes
```

### Frontend Environment (.env.local)
```bash
NEXT_PUBLIC_API_URL=http://localhost:8000/api
NEXT_PUBLIC_LLM_SERVICE_URL=http://localhost:8002
```

> [!IMPORTANT]
> The `NEXT_PUBLIC_API_URL` must include the `/api` prefix to match the backend URL configuration.


---

## üìä Development Workflow

### Git Workflow
1. **Create Feature Branch**: `git checkout -b feature/your-feature-name`
2. **Implement Changes**: Follow coding standards and add tests
3. **Commit Changes**: Use conventional commits (feat:, fix:, docs:)
4. **Push & PR**: Open pull request with detailed description
5. **Code Review**: Address feedback and merge

### Code Quality Standards
- **Python**: Black formatting, isort imports, flake8 linting
- **TypeScript**: ESLint, Prettier, strict type checking
- **Testing**: Minimum 80% code coverage required
- **Documentation**: Inline comments and README updates

---

## ü§ù Contributing

We welcome contributions! Please follow these guidelines:

1. **Fork** the repository and create your feature branch
2. **Follow** the existing code style and conventions
3. **Add tests** for new functionality
4. **Update documentation** as needed
5. **Submit** a pull request with a clear description

### Development Guidelines
- Use conventional commit messages
- Write comprehensive tests
- Update documentation for new features
- Follow existing architectural patterns
- Ensure all CI checks pass

---

## üìà Roadmap

### Phase 3 (Next Phase - Manual Data Collection)
- [ ] Research and collect high-quality training data for target domains
- [ ] Validate and import training data using local JSON storage
- [ ] Train initial domain-specific models
- [ ] Test and refine content generation quality

### Phase 4 (Q1 2025)
- [ ] Rich text editor integration
- [ ] AI-powered cover generation
- [ ] Advanced PDF customization
- [ ] Multi-language support

### Phase 5 (Future)
- [ ] Payment and subscription system
- [ ] Collaboration features
- [ ] Mobile application
- [ ] Enterprise features

---

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **OpenAI** for transformer architecture inspiration
- **Hugging Face** for the transformers library
- **MongoDB** for Atlas database services
- **Vercel** for Next.js framework
- **Django Software Foundation** for the web framework

---

**Made with ‚ù§Ô∏è by Badr Ribzat**

*Last updated: January 6, 2026*

